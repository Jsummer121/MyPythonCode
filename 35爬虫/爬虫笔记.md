# 爬虫

# 注：一般用<http://httpbin.org/headers>来测试自己的代码

## 一、HTTP与HTTPS协议

### 1.报文格式：

![1570950037985](assets\1570950037985.png)

GET / HTTP/1.0\r\nHost: www.baidu.com\r\n\r\n

 **/：这里的url表示百度的根目录**

**协议版本：HTTP/1.0  HTTP/1.1**



GET:请求指定页面信息，获取数据

​        协议   域名（ip地址+端口）   端口 

URL：https://www.baidu.com:80

​		 : https://tieba.baidu.com/index.html

ssh   22

HTTPS 443(在HTTP协议之上添加了SSL协议加密技术)

1-1024

65535



POST:发送数据，GET：请求正文

### 2.HTTP响应

![1570950972038](assets\1570950972038.png)

状态码：

200：ok  正常响应

403：拒绝请求

300：重定向--页面找不到，回到某个特定的网页

404：找不到页面

500 服务器内部错误

##### 添加： 请求头

![1570951938253](E:\python-summer-1\35爬虫\assets\1570951938253.png)

![1570951946113](assets\1570951946113.png)

![1570951952947](assets\1570951952947.png)

**一般网页判断** 

user-agant表示客户端信息

referer 查看页面由哪个页面产生

host 请求参数的域名

Accept-Encoding 当前网站是否支持gzip（一般不加）



#### 特征：

1.无连接

2.媒体独立

3.无状态，对于食物处理无记忆能力

## 二、socket

是一个标准库

网络通信

发送和接受，所有数据都是bytes的数据



1.导入模块

2.创建一个客户端

3.连接（ip地址和端口）（'ip地址/www.baidu.com','80'）

4.发送消息给百度

这里他需要的是bytes不需要字符串，所以需要转码

```
client.send(request.encode())#send发送
recv读取 client.recv(1024)#随便发送消息他不会回复，因此这里阻塞了
因为不能确定回复的由多上，所以选择循环接收。
```

因为图片是二进制文件，所以不能decode

5.图片如果直接放到本地，会打不开，因为多了请求头部，需要用正则去掉。

```
image_content = re.findall(b'\r\n\r\n(.*)',res,re.S)

print(image_content)

# 把图片写入到本地
with open('capt.jpg','wb') as f:
    f.write(image_content[0])
```

re.S 表示：正则里面.元字符能比配元字符，不加就不能匹配换行符。

心得：

连接百度的地址为你想爬取的图片地址的首页

http://b-ssl.duitang.com/uploads/item/201807/27/20180727173028_cSAWz.thumb.700_0.jpeg

那么百度地址为b-ssl.duitang.com

然后后面的地址放到构造报文里面

```
request = 'GET /uploads/item/201807/27/20180727173028_cSAWz.thumb.700_0.jpeg HTTP/1.0\r\nHost: b-ssl.duitang.com\r\n\r\n'
```

GET 后面的/为地址后面的东西。并且后面的url也得改为百度地址。

**贪婪匹配和非贪婪匹配**

什么开头 .*? 什么结尾  #非贪婪模式

abc

a.*? 他获取的是空，非贪婪模式

a.* 全部获取

如果没有结尾就用 .*  #贪婪模式

**因此在不知道结尾的情况下用贪婪模式，但是知道结尾的时候，必须要是有非贪婪模式，比如获取图片url地址。**

## 三、urllib

是一个HTTP请求库

在发送网络请求后，都会返回一个urllib.response的对象。它包含了请求回来的数据。它包含了一些书写和方法。供我们处理返回的结果。

1.read():获取相应返回的数据，只能用一次

2.readline()： 读取一行

3.info()：获取响应的头信息

4.geturl()：获取访问的url

5.getcode()：返回状态码

### 3.1直接访问

```
import urllib.request

#发起请求
data = urllib.request.urlopen('https://www.taobao.com')#拿到的是urllib.response的对象,需要用read读取二进制码，可以解码。并且read只能读取一次
print(data.read().decode())
```

以上代码可以直接去访问baidu首页。但是如果遇到需要检查客户端信息的时候。如果通代码自己去请求，他的客户端信息是：python-urllib/3.7

urlopen里面自带的参数：

url：也可以传一个request对象

tata：数据 form

timeout：超时时间

### 3.2 加入请求头部进行访问

默认的urlopen不支持请求头部所以需要重新改。

**使用urllib.request.Request**

```
herders = {
    'user-agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36'
}
req = urllib.request.Request('https:www.jianshu.com',headers = herders) #实例
#发送请求
urllib.request.urlopen(req).read().decode()
```



### 3.3Cookies操作

urlopen不支持 代理 cookies这些功能

需要自己创建一个新的opener对象

```
#3.Cookies操作
from http import cookiejar
#创建一个cookies对象
cookies = cookiejar.CookieJar()

#创建一个cookies处理器
req = urllib.request.HTTPCookieProcessor(cookies)

#创建一个opener对象
opener = urllib.request.build_opener(req)

# data = urllib.request.urlopen('https://www,baidu.com')
# print(data.info())

data = opener.open('https://www,baidu.com')
print(data.info())
```

### 3.4代理

查看电脑在百度下的外网地址：

```
print(urllib.request.urlopen('http://httpbin.org/ip').read().decode())
```

吐过一个地址访问次数过多，可能网页会将这个ip暂停访问，这时就需要代理。

```
#代理
#创建代理
proxy = {'http':'221.6.32.206:41816'}

#创建一个代理处理器
proxies = urllib.request.ProxyHandler(proxy)
#创建一个opener对象
opener = urllib.request.build_opener(proxies)

print(opener.open('http://httpbin.org/ip').read().decode())
```

### 3.5url里的编码与解码

quote  unquote :字符串

parse_qs   urlencode ：字典

```
#url里的编码与解码
from urllib import parse

params = '夏天'
print(parse.quote(params)) #编码
#%E5%A4%8F%E5%A4%A9

print(parse.parse_qs('wd=%E5%A4%8F%E5%A4%A9'))#有键值对的返回

print(parse.unquote('%E5%A4%8F%E5%A4%A9'))#单独解码

print(parse.urlencode())#他可以接受字典
```

### 3.6.爬取图片

```
#爬取图片
req = urllib.request.urlopen('http://www.weimeitupian.com/wp-content/uploads/2019/06/20190604074219709.gif')
data = req.read()
with open('1.jpg','wb') as f:
    f.write(data)
```

## 四、urllib3

只要实例了，直接用request就可以返回需要的东西。

```
import urllib3
http = urllib3.PoolManager()

res = http.request('GET','https://www.baidu.com')
print(res.data.decode())
```

```
#用文件的形式下载
http = urllib3.PoolManager()
data = http.request('GET','http://httpbin/bytes/1024000')
print(data.data)
```

```
#文件上传
http = urllib3.PoolManager()
with open('capt.jpg','rb') as f:
    image_content = f.read()

r = http.requets('post','http://httpbin.org/post',body=image_content)
body传二进制
field传字典
```

```
#爬取图片
```

## 五、Requests

功能和urllib是一样的

### 5.1请求方式：

```
r = requests.get('http://httpbin.org/get')
r = requests.post('https://www.baidu.com')
```

### 5.2 响应

#### 1.text属性返回的是文本内容

#### 2.content属性 返回二进制内容

#### 3.json方法 对json数据进行解码

r.text

但有时候会乱码

r.content.decode()

原因：socket返回的数据是bytes，调用text属性时，requests内部进行自己转码。是基于http的头部进行猜测。

可以用r.encoding进行查看猜测解码的规则。

所以可以进行手动解码：r.endoding = 'UTF-8'

#### 补充：网页上的链接 ？ 后面的东西都是get请求的参数

传参一定得是关键字参数：以下为requests的关键字参数

```
:param method: method for the new :class:`Request` object.
:param url: URL for the new :class:`Request` object.
:param params: (optional) Dictionary, list of tuples or bytes to send
    in the query string for the :class:`Request`.
:param data: (optional) Dictionary, list of tuples, bytes, or file-like
    object to send in the body of the :class:`Request`.
:param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
:param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
:param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
:param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
    ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
    or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
    defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
    to add for the file.
:param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
:param timeout: (optional) How many seconds to wait for the server to send data
    before giving up, as a float, or a :ref:`(connect timeout, read
    timeout) <timeouts>` tuple.
:type timeout: float or tuple
:param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
:type allow_redirects: bool
:param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
:param verify: (optional) Either a boolean, in which case it controls whether we verify
        the server's TLS certificate, or a string, in which case it must be a path
        to a CA bundle to use. Defaults to ``True``.
:param stream: (optional) if ``False``, the response content will be immediately downloaded.
:param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
:return: :class:`Response <Response>` object
:rtype: requests.Response
```

#### 4.当表单里面使用同一个key时，data = (('key1':'data1'),('key1':'data2'))

他会自己将两个值放到同一个key下，变成{'key1' : ['data1','data2']}

```
data = (('key1','value1'),('key1','value2'))

print(requests.post('http://httpbin.org/post',data=data).cookies)
```

#### 5.查看响应头信息

```
#查看cookies
print(requests.post('http://httpbin.org/post',data=data).cookies)
#查看headers
print(requests.post('http://httpbin.org/post',data=data).headers)
```

#### 6.重定向  302  301 

```
#查看访问网址
r.url
#查看历史记录
r.history
#禁止重定向
r = requests.get('http://github.com',allow_redirects=False) # 禁止重定向
```

#### 7.大文件下载：

**流下载iter_content，**

**chunk_size=1024读取的单位**

```
import requests
url = 'http://httpbin.org/bytes/102400000'
r = requests.get(url,stream=True)
# print(r.content)
for chunk in r.iter_content(chunk_size=1024):
    print(chunk)
```

#### 8.session 会话对象  所以会话发出去的请求 会自动保持状态

```
#一般用这个来进行登录操作
session.post('fff',data=data)

session.get('')
```

##### 8.1.同一主机发送多个请求，重用TCP连接，使得连接加快

```
import requests
import time
import urllib.request

s = requests.session() # session 所有的api和requests都是一样的

start_time = time.time()
for i in range(50):
    r = s.get('https://www.baidu.com')
    # r = requests.get('https://www.baidu.com')
    # r = urllib.request.urlopen('https://www.baidu.com')

print('耗时:{}秒'.format(time.time()-start_time))
```

#### 9.cookies

```
headers = {
'Cookie':'SINAGLOBAL=8253644680747.205.1557837860157; SCF=An1Ehww5pL0ULFtScaZuhpWzuVdBKvppnWGDkdy-TjnQIprWRsWucuxz6XWoFhAYHatzl793eE_8PgAnGVHwVrI.; SUHB=0N2kKvtZt0_zHb; ALF=1572097686; SUB=_2A25wiLHGDeRhGeNI4loS9ibNzz2IHXVQct-OrDV8PUJbkNAKLVHFkW1NSBs-t2sIn2d28PJoYFCpJBfc1zQaEsbW; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5jEjLBHlCIhxk14.TPipEV5JpX5oz75NHD95QfSo.Re0qReKBpWs4DqcjRi--ciKnRiK.pi--fiKLhi-2Ri--Ri-2fi-isi--fi-2piK.Ri--Ni-82iKn4i--Xi-i2i-27wrQt; Ugrow-G0=9ec894e3c5cc0435786b4ee8ec8a55cc; wvr=6; YF-V5-G0=95d69db6bf5dfdb71f82a9b7f3eb261a; wb_view_log_5698368141=1920*10801; _s_tentry=www.baidu.com; UOR=,,www.baidu.com; Apache=686057968637.399.1571059662781; ULV=1571059662880:9:1:1:686057968637.399.1571059662781:1569504728437; YF-Page-G0=89906ffc3e521323122dac5d52f3e959|1571059729|1571059660; webim_unReadCount=%7B%22time%22%3A1571059823394%2C%22dm_pub_total%22%3A2%2C%22chat_group_client%22%3A0%2C%22allcountNum%22%3A38%2C%22msgbox%22%3A0%7D',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3'
}

r = requests.get('https://weibo.com/caizicaixukun',headers=headers)
r.encoding='utf-8'
print(r.text)
```

#### 10.代理

```
代理

proxy = {
    'http':'221.6.32.206:41816',
    'https':'221.6.32.206:41816'
}

print(requests.get('http://httpbin.org/ip',proxies=proxy).json())
```

#### 11.注解

wb是写入字节的意思 ，必须要！！！！！！

爬取图片需要用到wb，读取图片的字节码，不需要经过转化或者编译。

#### 12.实战1：百度贴吧

## 六、fidder：抓包工具

xhr表示ajax请求

蓝色：图片

紫色：css

绿色：js

ctrl+X:清屏

命令行工具：

select:搜索对应类型的session content-type

？：根据url搜索

= 状态码和类型

@：类似的根据url搜索

断点

因为之前是服务器直接与客户端相连，现在有了fidder的时候，客户端与服务器之间连接了fidder因此可以添加断点来阻断客户端与服务器之间的联系。并且可以修改请求和访问的数据。

在rules里的automatic里面

如果开了fidder，那么如果在pycharm里面运行代码的时候，需要在url的后面添加，verify=False来忽略证书验证。

### app抓包

1.fldder的tool工具勾选两个复选框（弄完后需要重启）

然后在cmd里面输入ipconfig可以查看自己的ip地址，然后在手机里输入http://ip:8888来访问这个页面，他会出现一个网址，这是一个下载证书的页面，点击fidderroot，然后设设置代理：在wifi里面打开自己的WiFi然后选择高级，在选择代理主机名为自己电脑的ip，然后代理端口为8888.

## 七、lxml

页面解析

爬虫我们需要抓取的知识某个网站或者某个应用的一部分内容

结构化  现有结构 后有数据 转字典处理

非结构化【html】   正则表达式 lxml bs4

lxml----》xpath：路径语言

/      根路径        

//    从整个文档搜索不考虑位置

.      当前节点

..     当前节点的父节点

路径表达式-----定位

注：文件名不能取以及存在的名字

​        浏览器返回的和代码返回的不一定相同

**text()#获取某一个节点下的文本**

**string()#获取某个节点下所有的文本**

page.xpath('//book[1]/author/.text()')

page.xpath('string(//book[1])')

```python
pip install -i(换镜像源) https://pypi.doubanio.com/simple
1.导入etree
from lxml import etree
import requests

html = requests.get('https://www.baidu.com').content.decode()

selector = etree.HTML(html)
print(selector.xpath('..'))#查看父节点
print(selector.xpath('.'))#查看当前节点
print(selector.xpath('/html/body/div'))#[<Element div at 0xd4ca58>]
print(selector.xpath('/html/body/div/text()'))#[' ',' ']因为text方法返回标签下的文本内容，并不返回标签本身一个/表示下一级，//表示当前
print(selector.xpath('/html/body/div//text()'))#[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '新闻', ' ', 'hao123', ' ', '地图', ' ', '视频', ' ', '贴吧', ' ', ' ', '登录', ' ', ' ', 'document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">登录</a>\');\r\n                ', ' ', '更多产品', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '关于百度', ' ', 'About Baidu', ' ', ' ', '©2017\xa0Baidu\xa0', '使用百度前必读', '\xa0 ', '意见反馈', '\xa0京ICP证030173号\xa0 ', ' ', ' ', ' ', ' ']
print(selector.xpath('//text()'))#[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '新闻', ' ', 'hao123', ' ', '地图', ' ', '视频', ' ', '贴吧', ' ', ' ', '登录', ' ', ' ', 'document.write(\'<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\'+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ \'" name="tj_login" class="lb">登录</a>\');\r\n                ', ' ', '更多产品', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '关于百度', ' ', 'About Baidu', ' ', ' ', '©2017\xa0Baidu\xa0', '使用百度前必读', '\xa0 ', '意见反馈', '\xa0京ICP证030173号\xa0 ', ' ', ' ', ' ', ' ']
print(selector.xpath('//div[@id="u-sp"]'))#[]因为打开百度，每个人展示的东西是不一样的，所以在选择的时候，需要考量-------可cookie有关。
```

```
print(selector.xpath('/book[price>35]'))#元素大于35
#包含于---模糊匹配
//div[contains(@id,'note')]
#有什么开头
starts-with
#多个属性组合
//input(@id="ffff" and @name="username")
```

post请求在网页里面是不能直接打开的。



**数据本身是不会无缘无故出现的，第一种是服务端返回的，第二种是js产生的**



请求：https://www.jianshu.com/trending_notes

自动请求：https://www.jianshu.com/?seen_snote_ids%5B%5D=44737027&seen_snote_ids%5B%5D=41279245&seen_snote_ids%5B%5D=44550901&seen_snote_ids%5B%5D=40875304&seen_snote_ids%5B%5D=41760550&seen_snote_ids%5B%5D=44368760&seen_snote_ids%5B%5D=41376851&page=2

## 八、bs4

xml居于c语言

bs4基于python

下载 pip install BeautifulSoup4

```
#导入
from bs4 import BeautifulSoup
```

一般需要指定解析器：默认html.parser

一般通过标签查找只查找第一个标签的内容如果需要查找所有的，需要用:find_all()

### 支持css操作

```
print(soup.select('div > a'))
print（soup.select('a > .mnav')
print(soup.select("a[class='mnav']"))
```

## 九、scrapy框架

requests 多线程 回调函数 callback

requests。get（）   #阻塞

scrapy 框架 自发呆了并发 去重 调度



python安装库

**https://www.lfd.uci.edu/~gohlke/pythonlibs/**



**基本应用：**

**1.项目方式运行，2.单个文件方式运行**

### 1.运行单个文件项目

1.先写好代码

```python
import scrapy

class Myspider(scrapy.Spider):
    name = 'spider1'

    start_urls = ['https://www.baidu.com']

    def parse(self, response):
        """
        处理response的方法
        :param response: 
        :return: 
        """
        print(response,'======')
```

2.先打开命令行，然后cd到项目路径下

---cd 35爬虫/上课文件

---scrapy runspider 上课敲：scrapy.py 

### 2.项目方式

注：因为之前用requests库的时候是直接url访问的 ，并不会访问robot。txt文件，所以是可以直接访问的，但是scrapy是框架，亚需要遵守一些约定，所以他回去请求robot文件夹，这时是不能被访问的：会出现forbidden，这时需要在srettings文件里的ROBOTSTXT_OBET改为False

**1.创建项目**

**scrapy startproject project_name**

**2.创建一个爬虫文件**

**scrapy grnspider example example.com**

**3.运行爬虫**

**scrapy crawl example[运行的是文件上的命名的name属性值]**

**4.查看可以运行的爬行文件** 

**scrapy list**


### 3.scrapy 组件

spiders 爬虫程序   处理response 提取需要的数据

engine  引擎  不同组件之间的通信

scheduler 调度器 接收requests请求 排队加入队列【16个线程】

download 下载器  负责引擎发送过来的requests请求  进行下载

item pipeline 管道 负责spider  

### 4.回顾yield

**return是以此返回所有数据，一般使用yield来使数据一次一次返回。**

return 返回一个结果

yield 函数或者方法出现yield关键字，他就变成了生成器

### 5.案例：腾讯课堂

第一步：创建项目

scrapy startproject tencent

第二部：创建爬虫文件

cd tencent

scrapy genspider tencent_spider ddddddd

第三部：定义item文件

打开iten.py文件

```python
title = scrapy.Field()
link = scrapy.Field()
image_url = scrapy.Field()
```

第四部：解析网页数据 编写spider文件

```python
title_list = response.xpath('//ul[@class="course-card-list"]/li/h4/a/text()').extract()#.extract()把select对象序列化成字符串
link_url_list = response.xpath('//ul[@class="course-card-list"]/li/h4/a/@href').extract()
image_url_list = response.xpath('//ul[@class="course-card-list"]/li/a/img[@class="item-img"]/@src').extract()
```

第五步：返回数据

```python
for title,link_url,image_url in zip(title_list,link_url_list,image_url_list):
    items = TencentItem()
    items['title'] = title
    items['link_url'] = link_url
    items['image_url'] = image_url
    yield items
```

## 十、spider

最初始的request请求来自于start_urls的请求。

parse默认的回调方法，在子类中必须要重写。

start_requests 生成Request交给引擎下载返回resquests

**name	start_urls	start_requests	parse**

## 十一、CrawlSpiders：派生类

Spider 匹配url然后返回request请求

CrwalSpiders    url规则 自动生成Request请求

### 1.参数

LinkExtractor 作为一个类，

参数：allow 正则表达式 满足的url 会被提取出来

deny 正则表达式 满足的url不会被提取出来

restrict_xpaths 路径表达式 符合路径的标签提取出来

Rule 

参数：link_extractor 提取连接的实例对象

callback 回调函数

### 2.步骤

#### 第一步创建爬虫文件夹

**正常情况下：scrapy genspider baidu_text ddddc   #创建爬虫文件**

**现在：scrapy genspider -t crawl baidu_text dddd**

#### 第二步写入东西

```
start_urls = ['https://www.baidu.com']

rules = (
    Rule(LinkExtractor(allow=r'http.*?',restrict_xpaths='//a'), callback='parse_item', follow=True),
)

def parse_item(self, response):
    print(response.url,'======')
    item = {}
    #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
    #item['name'] = response.xpath('//div[@id="name"]').get()
    #item['description'] = response.xpath('//div[@id="description"]').get()
    return item
```

注：rules里面可以写入多个东西，他会一次一次的从上往下先后查找

```
class BaiduTextSpider(CrawlSpider):
    name = 'baidu_text'
    # allowed_domains = ['dddd']
    start_urls = ['https://www.baidu.com']

    rules = (
        Rule(LinkExtractor(allow=r'http.*?'), callback='parse_item', follow=True),
        Rule(LinkExtractor(restrict_xpaths='//a'), callback='a_item', follow=True),
    )

    def parse_item(self, response):
        print(response.url,'======')
        item = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        return item

    def a_item(self, response):
        print(response.url, '======')
        item = {}
        return item
```

#### 第三部如果需要写入到文件里面，则需要在piplines和settings里面配置文件

pipline.py

```
class TencentPipeline(object):

    def __init__(self):
        self.f =open('result_json','w',encoding='utf-8')

    def process_item(self, item, spider):
        self.f.write(str(item)+'\n')
        return item


    def close_spider(self,spider):
        self.f.close()
```

settings.py

将这一行取消注释：激活

```
ITEM_PIPELINES = {
   'tencent.pipelines.TencentPipeline': 300,#后面的数字为执行先后顺序，数字小的先执行。
}
```

## 十二、Request类

自动去重   url的md5哈希值 执行去重。

meta 比较常用 在不同请求之间传参，类型是字典。

**因为第一个访问的页面是首页，还有存在其他的页面导致一些数据不能一次性拿取到，所以这边需要重新定义新的方法和运用到meta来进行传参，并且这是一个字典的类型。**

**如果使用self.items，因为scrapy是多线程的所有线程同步执行，所以你不知道url所对应的price是否是正确的，所以需要进行传参来进行。**

**正常：**

```
for title,link_url,image_url in zip(title_list,link_url_list,image_url_list):
    items = TencentItem()
    items['title'] = title
    items['link_url'] = link_url
    items['image_url'] = image_url
    yield items
```

**需要再次传参：**

```
for title,link_url,image_url in zip(title_list,link_url_list,image_url_list):
        items = TencentItem()
        items['title'] = title
        items['link_url'] = link_url
        items['image_url'] = image_url
        yield scrapy.Request(url = link_url,callback=self.parse_price,meta={'items':items})
        
def parse_price(self,response):
    items = response.meta.get('items')
    price = 9999
    items['price'] = price
    yield items
```

### 1.去重：dont_filter=False

```
def parse(self, response):
    print(response,'=========')
    yield scrapy.Request(url='https://www.baidu.com',dont_filter=True)
```

### 2.errback如果请求出现错误，回调的方法。

### 3.priority 优先级

### 4.Resopnse

### 5.post请求：scrapy.FormRequest

给url和formdata

## 十三、日志处理

5个等级：DEBUG--error--warming--info

### 第一步

在setting.py里面写入

```
LOG_FILE = 'baidu.log'
LOG_LEVEL = 'ERROR'
```

如果把第一行注销掉，那么在crwal运行起来之后，他只会返回<200 https://www.baidu.com> ========

一般可以用日期存储法：

```
import datatime
now = datatime.datatime.now()
LOG_FILE = 'baidu-{}-{}-{}.log'.format(now.year,now.month,now.day)
LOG_LEVEL = 'ERROR'
```

**当出现error错误时，整个日志便会存储到这个文件里面，这样就可以根据文件的日期来查看相应的日期。**

### 第二步：运行

## 十四、模拟登陆

### 第一步：

观察网站--抓包

#### 1.1：fiddler

#### 1.2：无痕窗口打开想要的东西

找post请求的url

分析提交的参数

**1.找固定的参数**

**2.找变化的参数出现的位置**

构造data数据包

```python
formdata = {
    'commit':'Sign in',
    'utf8':'✓',
    'authenticity_token':authenticity_token,
    'ga_id':'10379410.1563805341',
    'login':'jxxaizwt@icloud.com',
    'password':password,
    'webauthn-support':'supported',
    'webauthn-iuvpaa-support':'unsupported',
    'required_field_71b9':'',
    'timestamp':str(int(time.time()) * 1000),#时间戳
    'timestamp_secret':timestamp_secret,
}
```

发送post请求

```
yield scrapy.FormRequest(url='https://www.github.com/session',formdata=formdata)
```

检查是否登陆成功

```
def login_alter(self,response):
     if 'Jsummer121' in response.text:
         print('登录成功')
```

## 十五、中间件

对请求和响应进行定制化的修改---修改reqeusts请求

### 1.下载中间件

process_request

process_response

下载器：

1.无法执行js代码。导致很多数据无法获取。

2.本身不支持代理

**当在middlewares.py修改代码之后需要在settings里面进行激活代码。**

**注意，激活的代码是**：

```
# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
   'tencent.middlewares.TencentDownloaderMiddleware': 543,
}
```

**下面这行代码写在class前面**

```
import base64

# 代理服务器
proxyServer = "http://http-pro.abuyun.com:9010"

# 代理隧道验证信息
proxyUser = "H6KYU825GIZVXOYP"
proxyPass = "C7B4B8D030FCBB27"

# for Python3
proxyAuth = "Basic " + base64.urlsafe_b64encode(bytes((proxyUser + ":" + proxyPass), "ascii")).decode("utf8")
```

**写在process_request行：**

```
def process_request(self, request, spider):
    # Called for each request that goes through the downloader
    # middleware.

    # Must either:
    # - return None: continue processing this request
    # - or return a Response object
    # - or return a Request object
    # - or raise IgnoreRequest: process_exception() methods of
    #   installed downloader middleware will be called
    request.meta["proxy"] = proxyServer
    request.headers["Proxy-Authorization"] = proxyAuth
```

**检查代码，是否影响成功**

**注意：start_urls必须是这个，不然可能导致中间件不能下载**

```
start_urls = ['https://httpbin.org/ip']

def parse(self, response):
    print(response.text,'=====================')
```

### 二、scrapy连接selenium

测试链接：www.aqistudy.cn/historydata/daydata.php?city=成都&month=201404

1.数据不是网页源代码返回的

2.请求参数和返回的数据都是经过加密的

3.一个页面请求只能找到一次

**seleium:测试自动化组件 驱动浏览器**

第一步：安装selenium

第二部：下载驱动：<http://chromedriver.storage.googleapis.com/index.html>

第三部：下载好后，把文件放到python根目录下。

应用：

当创建了scrapy的时候，selenium需要放在下载中间件来使用。

from selenium import webdriver

selenium使用的是代码所以获取网页代码的速度非常快，但是如果网页时ajax的，会使js还没导入，整个过程就结束了，所以这里需要使用time.sleep(2)在打开指定浏览器之后。

但是selenium会吧scrapy的异步进行阻塞，使他运行非常慢。这里使用无头浏览器

```
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

#打开浏览器
option = Options()
option.add_argument('--headless')# 隐身模式 节省内存
driver = webdriver.Chrome(chrome_options=option)

#打开指定的页面
driver.get('https:www.baidu.com')
```

## 十六、scrapy_redis(未仔细看，下次回看)

scrapy 通用的爬虫框架。

调度器  队列 所有请求都是在队列当中的

如何实现队列共享   redis   

管道涉及数据存储。

组件：scrapy_redis

自己创建scrapy_redis

首先scrapy项目

修改settings默认的调取器和过滤器

在spider里面修改集成的类RedisSpider   redis_key

### 改写scrapy_redis

1.在settings里面激活scrrapy_redis的组件调度器   过滤器  保持队列

```
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
```

#Redis数据库配置

REDIS_HOST = '127.0.01'

REDIS_POST = 6379

配置完以后 导入scrapy_redis

```
from scrapy_redis.spiders import RedisCrawlSpider,RedisSpider
```

```
redis_key = 'jianshu:start_urls'
```

DOWNLOAD_DELAY = 3#设置请求间隔

在settings里面打开pipline然后在

pipline里面设置：

导入pymongo

### 注：当数据库连接不上时，需要先修改配置文件

**退出环境：deactivate**

**查看路径下的所有文件：ls**

退出文件夹：  ：wq

**当mongo或者其他数据库连接不上时，需要修改配置文件。**

**进入虚拟机然后cd /etc/**

**然后进入配置文件：sudo vim mongodb.conf**

**把绑定的blind_ip地址换成‘0,0,0,0’**

**然后重启服务器：service mongodb restart**

## 实战1：微博登录

再按f12后，到network里面，因为有时候后面的请求会覆盖前面，所以在请求时需要把preserve log打上勾，这样后面的请求就不会吧前面的覆盖了。

1.先创建新浪的文件夹

2.创建scrapy的文件夹

scrapy startproject SinaSpider

3.创建spider文件

scrapy genspider Sina dddddd

4.因为在请求前就需要登录所以有两种方法1.在中间件里写，2.单独写文件cookies.py

并且登录有两种方法：1.requests。2.selenium

5.以上：登录和cookie的存储

6.发起请求

个人用户数据解析

7.在中间件里面加入一些自己的东西

### 思路

scrapy爬虫

请求和解析

直接请求 不行 验证 模拟登陆

## 实战2，空气质量网--断点

注：一般的网站数据的都经过加密，在处理这些数据的时候，需要经过断点。

一般情况下

1.先打开检查，然后清空当前的所有数据，在Application里面的local Stirage里面吧当前的缓存清空

2.再次请求，网页就会发起一个post请求，他会发送回一个参数，但是是经过加密的。

3.如果需要断某一个请求，先点击他的发起地址，然后点击下面的{}（格式化），

**需要：pyexecjs 执行js代码的模块**

**nodejs**

***虚拟机安装包的命令：sudo apt-get install***

#### 1.拿到请求回来的数据

#### 2.寻找解密的js代码。然后复制到一个js文件里面等待读取

#### 3.传参来获得解码

## 实战3.网易云和美拍js断点

1.百度搜索网易云，进入网易云页面

2.随便选择一个免费的歌曲

3.打开检查，点击network

4.在点击播放，寻找返回url地址的文件

**V1？csrf_token=**这个文件

5.找到后，获取他的参数

6.然后访问他的请求网站。

然后根据具体的想，去找解析的js代码，把他转换成python的格式就可以

## 实战4：微博数据解析

## scrapyd部署

把代码推送的服务器

安装：

pip install scrapyd            #安装客户端    安装在服务器



pip install scrapud-client #安装服务端   本地

x修改scrapy.cfg文件

在项目目录下的cfg文件下，将url激活然后将localhost改为127.0.0.1

上传

scrapyd-deploy <targert> -p <projectname>

target:deploy后面的内容可以自己设定--[deploy:1111]则target为1111



启动爬虫命令

curl http:127.0.0.1:6800/schedule.json -d project=SinaSpider -d spider=Sina



停止爬虫运行

curl http:127.0.0.1:6800/cance.json -d project=SinaSpider -d job=jop_id





