## 分类问题（上）

分类问题是监督学习的核心，他从出具中学习一个分类决策函数或分类模型，对新的输入进行输出预测，输出变量取有限个离散值。

核心算法：贝叶斯，SVM，逻辑回归，决策树

### 1.决策树

决策树是一个树的结构，每个非叶节点单表示一个特征属性，每个分支边代表这个特征属性在某个值域上的输出，每个叶节点存放一个类别。

决策过程：从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，知道到达叶子节点，将叶子节点存放的类别作为决策结果。

例如：根据西瓜的特点（纹理，根蒂），来判断是否是好瓜

**如何构建决策树？**

1.特征选择：选取对训练数据具有分类能力的特征

2.决策树生成：在决策树各个点上按照一定方法选择特征，递归构建决策树

3.决策树剪枝：在已生成的树上减掉一些子树或者叶节点，从而简化分类树的模型。

**核心算法**：

ID3算法，C4.5算法，CART算法

**决策树的特征选择**：

决策树构建过程中的特征选择是非常重要的一步。特征选择是决定用哪个特征来划分特征空间，特征选择是要选出对训练数据集具有分类能力的特征，这样可以提高决策树的学习效率。

信息熵：表示随机变量的不确定性，熵越大不确定性越大

信息增益：信息增益=信息熵（前）-信息熵（后）

信息增益比：信息增益比=惩罚参数*信息增益。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。

基尼指数：表示集合的不确定性，基尼系数越大，表示不平等程度越高。

**决策树剪枝**：

在生成树的过程中，如果没有剪枝操作，就会生成一个队训练集完全拟合的决策树，但这是对测试集非常不友好的，泛化能力不行。因此，需要减掉一些职业，使得模型泛化能力更强。【理想的决策树有三种：叶子节点数最少、叶子节点深度最小、叶子节点数量最少且叶子节点深度最小】

**方法：**

1.预剪枝：

​	通过提前停止数的构建而对树剪枝，一旦停止，节点就是叶子，该叶子持有自己中最频繁的类。

- 定义一个高度，当决策树达到该高度时就停止生长
- 达到某个节点的实例具有相同的特征向量
- 定义一个阈值（实例个数，系统性能增益等）

2.后剪枝方法：

​		首先构造完整的决策树，然后对那些置信度不够的节点子树用叶子节点来代替，该叶子的类标号用该节点子树中最平凡的类标记。相比于预剪枝，这种方法更常用，因为在预剪枝方法中精确地估计何时停止树增长很困难。

### **2.贝叶斯分类**：

贝叶斯分类是基于贝叶斯定理和属性特征条件独立性的分类方法。

例子：春天是流行性感冒的高发季节，当家里人打了一个喷嚏，他们感冒的概率是多少？

1.**计算先验概率**：家人没有任何状况下可能感冒的概率是多少？

2.**为每个属性计算条件概率**：：如果家人感冒了，那么他会打喷嚏的概率是多少，如果他没有感冒。出现打喷嚏症状的概率有多少？

3.**计算后验概率**：根据1,2求解最终的问题，这才是拥有贝叶斯思想的你该做的分析。

贝叶斯公式：
$$
P(c|x)=\frac{P(x,c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}
$$

$$
P(类别|特征)=\frac{P(特征|类别)P(类别)}{P(特征)}
$$

但是这样的公式可能出现的一个情况是，当一个概率为0时，整个的概率也为0，出现了一票否决的情况，为了防止这样的情况出现我们引入**拉普拉斯修正**：
$$
先验概率拉普拉斯修正 P(c)=\frac{Dc}{D}\Rightarrow P(c)=\frac{Dc+1}{D+N}\\
条件概率拉普拉斯修正P(x_i|c)=\frac{D_{c,x_i}}{Dc}\Rightarrow P(x_i|c)=\frac{D_{c,x_i}+1}{Dc+N_i})
$$
**优点：**

算法逻辑简单，易于实现。

分类过程中失控开销小

**缺点：**

理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上总并非如此，这是因为朴素贝叶斯模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。