# 贝叶斯分类实战

## 一、分类基本概念

### 1.分类在数据挖掘中的定义：

- 分类就是把一些新的数据项映射到给定类别的中的某一个类别
- 分类属于有监督学习，与之相对应的是无监督学习，比如聚类
- 分类和聚类的最大区别在于，分类数据中的一部分的类别是已知的，而聚类数据的类别未知。

### 2.分类流程

**步骤一**、将样本转化为等维的数据特征（特征提取）。所有样本必须具有相同数量的特征。兼顾特征的全面性和独特性。

| 动物种类 | 体型 | 翅膀数量 | 脚的只数 | 是否产蛋 | 是否有毛 | 类别     |
| -------- | ---- | -------- | -------- | -------- | -------- | -------- |
| 狗       | 中   | 0        | 4        | 否       | 是       | 哺乳动物 |
| 猪       | 大   | 0        | 4        | 否       | 是       | 哺乳动物 |
| 牛       | 大   | 0        | 4        | 否       | 是       | 哺乳动物 |
| 麻雀     | 小   | 2        | 2        | 是       | 是       | 鸟类     |
| 天鹅     | 中   | 2        | 2        | 是       | 是       | 鸟类     |
| 大雁     | 中   | 2        | 2        | 是       | 是       | 鸟类     |

**步骤二**、选择与类别相关的特征（特征选择）。上面的翅膀数，脚的只数，是否产蛋为非常相关，种类，体型为部分相关，是否有毛为完全无关

**步骤三**、建立分了力模型或分类器

分类器通常可以看做一个函数，它把特征映射到类的空间上。
$$
\int\left(x_{i1},x_{i2},x_{i3},x_{i4},x_{i5}.....x_{in}\right)\rightarrow y_i
$$

### 3.分类的方法

​		常用的分类方法主要包括相似函数，关联规则分类算法，K邻近分类算法，决策树分类算法，贝叶斯分类算法和基于模糊逻辑，遗传算法，粗糙集和神经网络分类斯算法。

​		分类算法有很多种，都有各自的优缺点和应用范围，这里我们以贝叶斯分类算法为例。

## 二、贝叶斯分类的概述

### 1.背景：

​		贝叶斯分类基于贝叶斯定理，贝叶斯定理是由18世纪概率论和决策论研究者Thomas Bayes发明的，故用起名字命名为贝叶斯定理。

​		通过对分类算法的比较研究发现，朴素贝叶斯分类法可以与决策树和经过挑选的神经网络分类器相媲美.用于大型数据库,贝叶斯分类法也具有很高准确率和速度。

### 2.贝叶斯分类概述

**先验概率与后验概率**

- 先验概率：由以往的数据分析得到的概率
- 后验概率：得到信息之后再重新加以修正的概率

**贝叶斯理论**

- 简单地说，贝叶斯丁路基于假设的先验概率，给定假设下观察到不同数据的概率，提供了一种计算后验概率的方法。
- 在人工智能领域，贝叶斯方法是一种非常具有代表性的不确定性知识表示和推理方法。

### 3.贝叶斯定理

- P（A）是A的先验概率或边缘概率，之所以称为”先验“是因为他不考虑任何B方面的因素。
- P（A|B）是已知B发生后A的概率，也有与得自B的取值而被称作A的后验概率。
- P（B|A）是已知A发生后B的概率，也有与得自A的取值而被称作B的后验概率。
- P（B）是B的先验概率或边缘概率，也作标准化常量。

**贝叶斯定理公式**
$$
P（A|B）= P(A)\frac{P(B|A)}{P(B)}
$$
贝叶斯公式提供了从先验证概率P（A），P（B）和P（B|A）计算后验证概率P（A|B）的方法。

**朴素贝叶斯**

假设待分类的各个属性相互独立的情况下构造的分类算法就是朴素贝叶斯算法。

基本思想是：给定的待分类项X{a1，a2....an}，求解在此项出现条件下各个类别yi出现的概率，哪个P（Yi|X）最大，就把此待分类项归属到哪个类别。

### 4.四种贝叶斯分类器

Naive Bayes：朴素贝叶斯，假定各个特征变量x是相互独立的。

TAN：对朴素贝叶斯进行来了扩展，允许个特征变量所对应的节点构成一棵树

BAN：第TAN扩展，允许各个特征变量对所对应的节点间关系构成一个图，而不是树

GBN：一种无约束的贝叶斯网络分类器

### 5.贝叶斯分类的流程

- 准备阶段：主要是依据具体情况确定特征属性，并且对特征数学进行适当划分。然后就是对一部分待分类项进行人工划分，已确定训练样本。输入时所有的待分类项，输出是特征属性和训练样本。
- 分类器训练阶段：计算每个类别在训练样本中出现评率以及每个特征属性划分对每个类别的条件概率估计。输入时特征属性和训练样本，输出是分类器。
- 应用阶段：使用分类器所待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。

## 三、垃圾邮件分类实战

**训练数据**：垃圾邮件（0.txt至150.txt），其中 1-127位垃圾邮件，128至150位正常邮件，151-156位测试邮件。

**问题分析**：

1. 读取全部训练集，删除其中干扰字符，例如【】*。、，等等，然后分词，再删除长度为1的单个字，这样单个字对于文本分类没有贡献，剩下的词汇认为是有效词汇。
2. 统计全部训练集中每个有效词汇的出现次数，截取出现次数最多的前N个
3. 根据第一步预处理后的垃圾邮件和废垃圾邮件内容正是生成特征向量，统计第二部中得到的N个词语分别在该邮件中出现的频率。
4. 根据步骤三中得到的特征向量和已知邮件分类创建并训朴素贝叶斯模型
5. 读取测试邮件，参考第一步，对邮件文本进行预处理，提取特征向量
6. 使用第四步中训练好的模型，根据第五步提取的特征向量对邮件进行分类。



**代码**

```
from re import sub # 用来删除邮件的标点符号等其他无关的字符
from collections import Counter # 计算邮件中出现最多的词
from itertools import chain # 可以一次返回迭代对象
from numpy import array
from jieba import cut
from sklearn.naive_bayes import MultinomialNB

def getWoedsFromFile(txtFile):
    words = []
    with open(txtFile,encoding='utf8') as fp:
        for line in fp:
            line = line.strip()
            line = sub(r'[.【】 0-9\—。、！~\*]','',line)
            line = cut(line)
            line = filter(lambda word:len(word)>1,line)
            words.extend(line)
    return words

# 获取全部训练集中出现次数最多的词
allWoeds = []
def getTopNWords(topN):
    txtFiles = [str(i)+'.txt' for i in range(151)]
    for txtFile in txtFiles:
        allWords.append(getWoedsFromFile(txtFile))
    freq = Counter(chain(*allWords))
    return [w[0] for w in freq.most_common(topN)]

topWords = getTopWoeds(600)


# 获取特征向量，创建模型训练
vectors = []
for word in allWords:
    temp = list(map(lambda x:words.count(x),topWords))
vectors = array(vectors)
labels = array([1]*127 + [0]*24)

model = MultinomialNB()
model.fit(vectors,labels)

def predict(txtFile):
    words = getWoedsFromFile(txtFile)
    currentVector = array(tuple(map(lambda x: words.count(x),topWords)))
    result = model.predict(currentVector.reshape(1,-1))[0]
    return '垃圾邮件' if result == 1 else '正常邮件'

for mail in ('%d.txt'%i for i in range(151,156)):
    print(mail,predict(mail),sep=':')
```