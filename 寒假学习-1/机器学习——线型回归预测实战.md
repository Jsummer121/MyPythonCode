## 线型回归

分类——》离散的：明天的天气

回归——》连续的：房价的预测

### 一、原理：

**回归的由来：**

​		“回归”是由英国著名生物学家兼统计学家高尔顿(Francis Galton,1822～1911.生物学家达尔文的表弟)在研究人类遗传问题时提出来的。

​		为了研究父代与子代身高的关系，高尔顿搜集了1078对父亲及其儿子的身高数据。他发现这些数据的散点图大致呈直线状态，也就是说，总的趋势是父亲的身高增加时，儿子的身高也倾向于增加。但是，高尔顿对试验数据进行了深入的分析，发现了一个很有趣的现象—回归效应。因为当父亲高于平均身高时，他们的儿子身高比他更高的概率要小于比他更矮的概率；父亲矮于平均身高时，他们的儿子身高比他更矮的概率要小于比他更高的概率。

​		它反映了一个规律，即这两种身高父亲的儿子的身高，有向他们父辈的平均身高回归的趋势。对于这个一般结论的解释是:大自然具有一种约束力，使人类身高的分布相对稳定而不产生两极分化，这就是所谓的“回归效应”。



### 二、回归分析概念：

**回归分析法**：将指具有相关关系的两个变量之间的数量关系进行测定，通过建立一个数学表达式进行统计估计和预测的统计研究方法。

**自变量**：一般把作为估测依据的变量叫做自变量

**应变量**：待估测的变量

**回归方程**：反映自变量和因变量之间练习的数学表达式

**回归模型**：某一类回归方程的总称



### 三、回归分析原理：

#### 1.步骤：

1.测定相关关系的密切程度

2.建立回归方程

3.利用回归模型进行预测



#### 2.线型回归模型原理：

​		根据数学知识容易得知，对于平面上不重合的两点P（x1，y1）和Q（x2，y2），可以唯一确定一条直线。根据得到的公式就可以精确的计算任意x值在该直线上对应的y值。

​		如果平面上有若干样本点，并且这些点不贡献。现在要求找到一条最佳回归直线，使得这些点的总离差最小，确定最佳回归系数w，满足下面的公式
$$
\min_{\omega}||X\omega-y||_2^2
$$
​		其中，X为包含若干x坐标的数组，Xw为这些坐标在直线上对应点的纵坐标，y为样本点的实际纵坐标。

​		确定了最佳回归线方程之后，就可以对未知样本进行预测了额，也就是在计算任意x值在该直线上对应点的y值。



## 四、预测儿童身高实战

假定一个人的身高只受年龄、性别、父母身高、祖父母身高和外祖父母身高这几个因素影响，并假定大致符合线型关系。

在其他条件不变的情况下，随着年龄的增长会越来越高

同样，对于其他条件都相同的儿童，其父母身高较高的话，儿童也会略高一些

假定到18岁后身高长期保持固定而不再变化（不考虑年龄大了之后会稍微变矮一点）

根据给定训练数据和对应标签，线型拟合出儿童身高模型，预测测试数据儿童的身高。



训练数据：每一行表示一个样本，包含信息为：儿童年龄，性别（0女1男），父亲，目前，祖父，祖母，外祖父，外祖母身高

```
[[1,0,180,165,175,165,170,165],
[3,0,180,165,175,165,173,165],
[4,0,180,165,175,165,170,165],
[6,0,180,165,175,165,170,165],
[8,0,180,165,175,167,170,165],
[10,0,180,165,175,165,170,165],
[11,0,180,165,175,165,170,165],
[12,0,180,165,175,165,170,165],
[13,1,180,165,175,165,170,165],
[14,0,180,165,175,165,170,165],
[17,0,180,165,175,165,170,165]
]
```

对应标签数据：儿童身高

```
[60,90,100,110,130,140,150,164,160,163,168]
```

待预测数据：

```
[[10,0,180,165,175,165,170,165],[17,1,173,153,175,161,170,161],[34,0,170,165,170,165,170,165]]
```



```
import numpy as np
from sklearn import linear_model
import copy

# 准备训练数据
x = np.array([[1,0,180,165,175,165,170,165],[3,0,180,165,175,165,173,165],[4,0,180,165,175,165,170,165],[6,0,180,165,175,165,170,165],[8,0,180,165,175,167,170,165],[10,0,180,165,175,165,170,165],[11,0,180,165,175,165,170,165],[12,0,180,165,175,165,170,165],[13,1,180,165,175,165,170,165],[14,0,180,165,175,165,170,165],[17,0,180,165,175,165,170,165]])
y = np.array([60,90,100,110,130,140,150,164,160,163,168])

# 创建线型回归模型
lr = linear_model.LinearRegression()

# 训练模型
lr.fit(x,y)

# 准备待预测数据
xs = np.array([[10,0,180,165,175,165,170,165],[17,1,173,153,175,161,170,161],[34,0,170,165,170,165,170,165]])

# 预测数据并打印出结果
for item in xs:
    item1 = copy.deepcopy(item)

    if item1[0] > 18:
        item1[0] = 18

    print(item, ":", lr.predict(item1.reshape(1,-1)))

>>>
[ 10   0 180 165 175 165 170 165] : [136.20700438]
[ 17   1 173 153 175 161 170 161] : [172.41400876]
[ 34   0 170 165 170 165 170 165] : [191.65666041]
```